= Dynamic Elastic Multi-LoRA Model on-device Inference
Xihao Sun <sunxh2016@lzu.edu.cn>
:toc:
:icons: font
:url-quickref: https://docs.asciidoctor.org/asciidoc/latest/syntax-quick-reference/

== Problem Statement

我们收集并统计了三类应用当前最常使用的模型的大小，以及每类应用常用的基准测试的数据集。
我们发现，对于不同的应用，使用的模型大小也不同，对应的推理延迟和性能也不同。因此我们需要一种通用的模型，能够适应不同的应用，保证模型的延迟和性能诉求。

.Common Model Sizes Used for Different Applications
|===
|Application|Model Size|Models
|Chatbotfootnote:[https://lmsys.org/blog/2023-05-25-leaderboard/]|3B-7B|	Orac-mini-3Bfootnote:[Orca: Progressive Learning from Complex
Explanation Traces of GPT-4]/FastChat-T5-3B/Vicuna-7B/MPT-7Bfootnote:[https://lmsys.org/blog/2023-05-25-leaderboard/]/Llama2-7Bfootnote:[Llama 2: Open Foundation and Fine-Tuned Chat Models]/Falcon-7B
// |Translationfootnote:[Reseh Development of Machine translation and Large Language Model]footnote:[BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models]|6B-13B|ChatGLM-6B/Alpaca-7B/Vicuna-13B
|Code Generationfootnote:[https://zhuanlan.zhihu.com/p/651439303]footnote:[Large Language Models Meet NL2Code: A Survey]|13B-16B|CodeLlama-13Bfootnote:[1]/CodeT5P-16Bfootnote:[CodeT5+: Open Code Large Language Models for Code Understanding and Generation]/OctoCoder-16Bfootnote:[OctoPack: Instruction Tuning Code Large Language Models]
|High-quality Document Summarizationfootnote:[A Systematic Survey of Text Summarization: From Statistical Methods to Large Language Models]|>30B|Llama2-70Bfootnote:[Cross-lingual Multi-document Summarization Based on Chain-of-Thought]/GPT-4/PT-4footnote:[A Systematic Survey of Text Summarization: From Statistical Methods to Large Language Models]
|===

* QA：SQuADfootnote:[Squad: 100,000+ questions for machine comprehension of text], ARC easy and challenge (Clark et al., 2018)footnote:[Think you have solved question answering? try arc, the ai2 reasoning challenge]
// SQuAD prompt mean length: 13
// ARC-e prompt mean length: 23
// ARC-c prompt mean length: 26

// * Translation： Workshop on Machine Translation()

* Code. We report the average pass@1 scores of our models on HumanEval (Chen et al., 2021)footnote:[Evaluating large
language models trained on code] and MBPP (Austin et al., 2021)footnote:[Program synthesis with large language
models].
// HumanEval prompt mean length: 132

* Commonsense Reasoning.
// TrivialQAfootnote:[TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension]
L-Evalfootnote:[L-Eval: Instituting Standardized Evaluation for Long Context Language Models]
// TrivialQA prompt mean length: 14
// LEval prompt mean length: 19748

为了统计模型在不同的数据集上的推理延迟，我们统计了数据集中prompt的长度，并采用平均长度的数据进行平均推理延迟的测评。

.Performance and Latency Comparison of Different Models
|===
|Model|SQuAD 0-shot|TTFT|TPOT|ARC-e|TTFT|TPOT|ARC-c|TTFT|TPOT|Human-Eval pass@1|TTFT|TPOT|L-Eval ROUGE-1|TTFT|TPOT
// |Orac-mini-3B||||||||||41.55||
// |FastChat-T5-3B||||||||||||
// |Vicuna-7B|sq|t|t|e|t|t|c|t|t|h|t|t|28.91||
// |MPT-7B| 59.5|||70.2|||42.6|||18.3|||7.66||
|Llama-2-7B|67.2|||75.2|||45.9|||12.8|||29.75||
|Llama-2-13B|72.9|||77.3|43.942|26.409|49.4|45.512|26.23|18.3|130.77|26.74|30.49|8107.264|50.576
// |Vicuna-13B|sq|t|t|e|t|t|c|t|t|h|t|t|28.59||
|Llama-2-70B|80.7|||80.2|4089.338|1132.963|57.4|4808.393|1134.967|29.9|19766.63|1151.89|44.4312||
|===

== Existing Works
// 切换延迟
* Multi-Task-Multi-Models
* Multi-LoRA-Single-Model
// 不同稀疏性的剪枝
// 推理延迟TTFT
// 推理精度ELMS
* Multi-Task-Flexible-Model

** 模型切换延迟对比

.Comparison of Model Switching Latency
image::Figure/switching-latency.png[switching-latency]

// 除了精度，还要从别的方向喷ELMS

// 如果模型装载不下，应该如何处理？

// 多任务的kv cache 管理！

== Observations

== Challenges

== Design
* 切分剪枝
* 切换
* 多任务的kv cache 管理

== Experiments

=== Setup
* Hardware
** Nvidia Geforce RTX 3090
** Nvidia Geforce RTX 4090
** Nvidia Jetson AGX Orin 64 GB

* Software
** Llama-2-7B
** DeepSeek
** Qwen-7B

* Dataset
** Alpaca-en
** Wikitext-2

* Evaluation
** Perplexity
** Latency
** Memory Usage

=== Results
* Memory Analysis
* Performance Analysis
* Accuracy Analysis
* Resource Scheduling

== Conclusion

=== Existing Work
