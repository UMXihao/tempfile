= Dynamic Elastic Multi-LoRA Model on-device Inference
Xihao Sun <sunxh2016@lzu.edu.cn>
:toc:
:icons: font
:url-quickref: https://docs.asciidoctor.org/asciidoc/latest/syntax-quick-reference/

== Problem Statement

.Common Model Sizes Used for Different Applications
|===
|Application|Model Size|Models
|Chatbotfootnote:[https://lmsys.org/blog/2023-05-25-leaderboard/]|3B-7B|	Orac-mini-3Bfootnote:[Orca: Progressive Learning from Complex
Explanation Traces of GPT-4]/FastChat-T5-3B/Vicuna-7B/MPT-7Bfootnote:[https://lmsys.org/blog/2023-05-25-leaderboard/]/Llama2-7Bfootnote:[Llama 2: Open Foundation and Fine-Tuned Chat Models]/Falcon-7B
// |Translationfootnote:[Reseh Development of Machine translation and Large Language Model]footnote:[BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models]|6B-13B|ChatGLM-6B/Alpaca-7B/Vicuna-13B
|Code Generationfootnote:[https://zhuanlan.zhihu.com/p/651439303]footnote:[Large Language Models Meet NL2Code: A Survey]|13B-16B|CodeLlama-13Bfootnote:[1]/CodeT5P-16Bfootnote:[CodeT5+: Open Code Large Language Models for Code Understanding and Generation]/OctoCoder-16Bfootnote:[OctoPack: Instruction Tuning Code Large Language Models]
|High-quality Document Summarizationfootnote:[A Systematic Survey of Text Summarization: From Statistical Methods to Large Language Models]|>30B|Llama2-70Bfootnote:[Cross-lingual Multi-document Summarization Based on Chain-of-Thought]/GPT-4/PT-4footnote:[A Systematic Survey of Text Summarization: From Statistical Methods to Large Language Models]
|===

* QA：SQuADfootnote:[Squad: 100,000+ questions for machine comprehension of text], ARC easy and challenge (Clark et al., 2018)footnote:[Think you have solved question answering? try arc, the ai2 reasoning challenge]
// SQuAD prompt mean length: 13
// ARC-e prompt mean length: 23
// ARC-c prompt mean length: 26

// * Translation： Workshop on Machine Translation()

* Code. We report the average pass@1 scores of our models on HumanEval (Chen et al., 2021)footnote:[Evaluating large
language models trained on code] and MBPP (Austin et al., 2021)footnote:[Program synthesis with large language
models].
// HumanEval prompt mean length: 132

* Commonsense Reasoning.
// TrivialQAfootnote:[TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension]
L-Evalfootnote:[L-Eval: Instituting Standardized Evaluation for Long Context Language Models]
// TrivialQA prompt mean length: 14
// LEval prompt mean length: 19748

为了统计模型在不同的数据集上的推理延迟，我们统计了数据集中prompt的长度，并采用平均长度的数据进行平均推理延迟的测评。

.Performance and Latency Comparison of Different Models
|===
|Model|SQuAD 0-shot|TTFT|TPOT|ARC-e|TTFT|TPOT|ARC-c|TTFT|TPOT|Human-Eval pass@1|TTFT|TPOT|L-Eval ROUGE-1|TTFT|TPOT
// |Orac-mini-3B||||||||||41.55||
// |FastChat-T5-3B||||||||||||
// |Vicuna-7B|sq|t|t|e|t|t|c|t|t|h|t|t|28.91||
// |MPT-7B| 59.5|||70.2|||42.6|||18.3|||7.66||
|Llama-2-7B|67.2|18.638|10.797|75.2|21.639|10.777|45.9|21.711|10.792|12.8|72.79777778|11.09222222|29.75|2093.852222|16.47333333
|Llama-2-13B|72.9|37.69|24.562|77.3|43.942|26.409|49.4|45.512|26.23|18.3|130.77|26.74|30.49|8107.264|50.576
// |Vicuna-13B|sq|t|t|e|t|t|c|t|t|h|t|t|28.59||
|Llama-2-70B|80.7|4586.691|1126.855|80.2|4089.338|1132.963|57.4|4808.393|1134.967|29.9|19766.63|1151.89|44.4312|362433.67|950.06
|===

The experiments were run on a Quadro RTX 6000 equipped with 24GB of GDDR6 video memory, and since 24G could not accommodate the 70B model, CPU offloading of the computation had to be used, which ultimately led to a significant increase in the overall inference latency.

== Existing Works
// 切换延迟
* Multi-App-Multi-Models

In the event of the exclusive large model being customised for each task, it is necessary to load the model parameters from disk to CPU when loading to GPU each time the application is switched.
The following illustration shows the model inference latency for different sizes, considering the loading time of the model into TTFT:

.TTFT Considering the Loading Time
image::Figure/cpu-gpu-latency.png[switching-latency]

* Multi-App-Static-Model

The static model only satisfies a single inference delay or a single accuracy.

* Multi-App-Flexible-Model

ELMSfootnote:[ELMS: Elasticized Large Language Models On Mobile Devices] Using mobile phones as experimental platforms, it only supports the deployment of 3B-7B small and medium scale models, not large models. As for consumer-grade GPUs, the reasoning of 70B large models can be achieved with the help of quantisation and CPU offloading computation.

== Observations
* Interlayer importance
** 基于梯度
*** 梯度✖️激活值
- 计算目标输出对某层输出的梯度
- 将梯度与激活值相乘得到重要性分数
*** 梯度范数
- 计算每层参数梯度L2范数作为重要性指标
** 基于IO Similarity
- 层输入输出余弦相似度
- 计算相邻层输出分布的KL散度
** 基于扰动
- 噪声注入
- 层丢弃影响
** 基于注意力
- 评估注意力得分
** 基于权重
- 权重阈值分析

* Skipping whole FFN layers is more accurate than partial FFN parameters

 TODO

* The computational delay of FFN is less than that of MHA


== Challenges

== Design
* 切分剪枝
* 切换
* 多任务的kv cache 管理

== Experiments

=== Setup
* Hardware
** Nvidia Geforce RTX 3090
** Nvidia Geforce RTX 4090
** Nvidia Jetson AGX Orin 64 GB

* Software
** Llama-2-7B
** DeepSeek
** Qwen-7B

* Dataset
** Alpaca-en
** Wikitext-2

* Evaluation
** Perplexity
** Latency
** Memory Usage

=== Results
* Memory Analysis
* Performance Analysis
* Accuracy Analysis
* Resource Scheduling

== Conclusion

=== Existing Work
