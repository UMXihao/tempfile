
./llama-cli -m ../models/Llama-2-7b-hf-gguf/Llama-2-7b-hf-gguf.gguf -n 128 -p "What happens to you if you eat watermelon seeds?" --split_mode row --tensor_split 1

./llama-server -m ../models/Llama-2-7b-hf-gguf/Llama-2-7b-hf-gguf.gguf --split_mode row --tensor_split 1 --port 8080

原始模型全部卸载到GPU的计算结果：
./llama-cli -m ../../models/Llama-2-7b-hf-gguf/Llama-2-7b-hf-gguf.gguf -n 128 -p "What happens to you if you eat watermelon seeds?" -ngl 33

llama_perf_print:    sampling time =       3.86 ms /   143 runs   (    0.03 ms per token, 37065.84 tokens per second)
llama_perf_print:        load time =   19644.05 ms
llama_perf_print: prompt eval time =      33.50 ms /    15 tokens (    2.23 ms per token,   447.75 tokens per second)
llama_perf_print:        eval time =    3538.83 ms /   127 runs   (   27.86 ms per token,    35.89 tokens per second)
llama_perf_print:       total time =    3582.87 ms /   142 tokens


当前并行结算的时间花销：
llama_perf_print:    sampling time =       3.83 ms /   143 runs   (    0.03 ms per token, 37366.08 tokens per second)
llama_perf_print:        load time =   19679.59 ms
llama_perf_print: prompt eval time =      33.45 ms /    15 tokens (    2.23 ms per token,   448.42 tokens per second)
llama_perf_print:        eval time =    3977.70 ms /   127 runs   (   31.32 ms per token,    31.93 tokens per second)
llama_perf_print:       total time =    4032.56 ms /   142 tokens
